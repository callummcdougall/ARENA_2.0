# %%

# imports

import sklearn
import streamlit as st
import openai
from openai.embeddings_utils import distances_from_embeddings
import torch as t
import pandas as pd
import tiktoken
from pathlib import Path
import os
from tqdm.notebook import tqdm
import torch as t
from typing import List, Dict, Union, Callable
import pickle
from dataclasses import dataclass
import time
tokenizer = tiktoken.get_encoding("cl100k_base")

# Get to chapter2_rl directory (or whatever the chapter dir is)
import os, sys
CHAPTER = r"chapter2_rl"
chapter_dir = r"./" if CHAPTER in os.listdir() else os.getcwd().split(CHAPTER)[0]
sys.path.append(chapter_dir + CHAPTER)

# from exercises.plotly_utils import hist

openai.api_key = st.secrets["openai_api_key"]

SEPARATOR = "\n" + "=" * 30 + "\n"

@dataclass
class Embedding:
    '''
    Class for holding a single chunk of text, and its embedding vector.
    Also includes titles which are things like "[0.1] Optimization - 045 (solutions)", used for filtering.
    '''
    title: str = ""
    text: str = ""
    embedding_vector: t.Tensor = t.tensor([])
    n_tokens: int = 0

    def __post_init__(self):
        self.embedding_vector = t.tensor(openai.Embedding.create(
            input=self.text, 
            engine='text-embedding-ada-002'
        )['data'][0]['embedding'])
        self.n_tokens = len(tokenizer.encode(self.text, allowed_special={'<|endoftext|>'}))


class EmbeddingGroup:
    def __init__(self, embeddings: List[Embedding] = []):
        self.embeddings = embeddings

    def add_embedding(self, title: str, text: str):
        self.embeddings.append(Embedding(title, text))

    def __getitem__(self, idx):
        return self.embeddings[idx]
    
    def __iter__(self):
        return iter(self.embeddings)
    
    def __len__(self):
        return len(self.embeddings)

    @classmethod
    def from_chunked_files(self, filenames: List[Path]):
        '''
        Creates embeddings from a list of chunked files (e.g. chunk_Ray_Tracing.txt, ...).

        Appends `(solution)` to title of embedding if it's detected to be a solution.
        '''
        all_titles_and_text = []
        for filename in filenames:
            file_text = filename.read_text()
            chunks = file_text.split(SEPARATOR)
            for idx, chunk in enumerate(chunks):
                if chunk.strip() != "":
                    title = f"{filename.stem.replace('chunk_', 'chunk: ')} [{idx}]"
                    if "Solution:" in chunk:
                        title += " (solution)"
                    all_titles_and_text.append((title, chunk))

        e = EmbeddingGroup()
        t0 = time.time()
        bar = tqdm(all_titles_and_text)
        for title, text in bar:
            e.add_embedding(title, text)
            if time.time() - t0 > 0.5:
                bar.set_description(f"Processing {title!r}")
                t0 = time.time()
        return e

    # def hist(self, **kwargs):
    #     hist(
    #         self.n_tokens, 
    #         labels={"x": "Number of tokens", "y": "Chunk freq"},
    #         **kwargs,
    #     )

    def save(self, path: Union[str, Path] = "my_embeddings.pkl"):
        with open(path, 'wb') as f:
            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)

    def filter(
            self, 
            title_filter: Callable = lambda x: True, 
            text_filter: Callable = lambda x: True,
        ) -> "EmbeddingGroup":
        '''
        Returns a new embedding group with a filter applied.

        Useful for e.g. removing code or choosing only certain sections of material.
        '''
        def embedding_filter(e: Embedding):
            return title_filter(e.title) and text_filter(e.text)
        return EmbeddingGroup(list(filter(embedding_filter, self.embeddings)))

    @property
    def titles(self):
        return [e.title for e in self]

    @property
    def texts(self):
        return [e.text for e in self]

    @property
    def embeddings_tensor(self):
        return t.stack([e.embedding_vector for e in self])

    @property
    def n_tokens(self):
        return t.tensor([e.n_tokens for e in self])

    # @classmethod
    # def load(cls, path: Path = Path("my_embeddings.pkl")) -> "EmbeddingGroup":
    #     '''
    #     Gets an embedding group from a file (where it's been saved).
    #     '''
    #     with path.open("rb") as f:
    #         embs = pickle.load(f)
    #     return embs


def create_context(
    question: str, 
    my_embeddings: EmbeddingGroup, 
    max_len: int, 
    engine: str,
    debug: bool,
):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """
    # Get the embeddings for the question
    q_embeddings = openai.Embedding.create(input=question, engine=engine)['data'][0]['embedding']

    # Get the distances from the embeddings
    embedding_distances = distances_from_embeddings(q_embeddings, my_embeddings.embeddings_tensor, distance_metric='cosine')

    if debug: st.markdown(f"""## Useful info
number of embeddings: {my_embeddings.embeddings_tensor.shape}
number of distances = {len(embedding_distances)}
""")

    returns = []
    cur_len = 0
    n_continues = 0

    df = pd.DataFrame({
        'text': my_embeddings.texts,
        'distances': embedding_distances,
        'n_tokens': my_embeddings.n_tokens
    })

    # Sort by distance and add the text to the context until the context is too long
    for i, row in df.sort_values('distances', ascending=True).iterrows():
        
        # If the context is too long, continue (we do this a max of 5 times, just to make sure we aren't terminating early because of an unusually long chunk)
        if cur_len + row['n_tokens'] + 4 > max_len:
            # Add the length of the text to the current length (+4 for the ###\n\n)
            n_continues += 1
            if n_continues > 5: break
            continue
        # Else add it to the text that is being returned
        else:
            cur_len += row['n_tokens'] + 4
            returns.append((i, row["text"]))
        

    returns = "\n\n###\n\n".join([
        text for i, text in sorted(returns, key=lambda x: x[0])
    ])
    return returns


prompt_templates_dict = {
    "SIMPLE": """
Try to answer the question based on the context below. If the question can't be answered based on the context, say \"I don't know how to answer that based on the context from this course, but I'll still try to answer.\", then answer the question like you normally would.\n\nContext: {context}\n\n---\n\nQuestion: {question}\nAnswer:
""",

    "COMPLEX": """
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:

Question: [question here]

Helpful Answer: [answer here]

Score: [score between 0 and 100]

Begin!

Context:
---------
{context}
---------

Question: {question}

Helpful Answer:
"""
}


def answer_question(
    my_embeddings: EmbeddingGroup,
    model: str = "text-davinci-003",
    question: str = "What is an example question which you can answer for me?",
    prompt_template: str = "SIMPLE",
    prompt_templates_dict: Dict[str, str] = prompt_templates_dict,
    max_len: int = 1800,
    engine: str = 'text-embedding-ada-002',
    max_tokens: int = 200,
    stop_sequence = None,
    debug: bool = False,
    temperature: float = 0.5,
    container = None,
):
    """
    Answer a question based on the most similar context from the dataframe texts
    """
    # print("creating context")
    context = create_context(
        question,
        my_embeddings,
        max_len=max_len,
        engine=engine,
        debug=debug
    )
    if debug: st.markdown(
f"""## Context

{context}
""")
    # print("creating answer")
    prompt_template = prompt_templates_dict[prompt_template]
    prompt = prompt_template.format(question=question, context=context)
    if debug:
        print(f"Context length: {len(context)}")
        print("\n\n")
        print(f"Responding to prompt:\n\n{prompt}")
    kwargs = dict(
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0,
        stop=stop_sequence,
        model=model,
        stream=True
    )
    if model in ["text-davinci-003"]:
        create_func = lambda prompt: openai.Completion.create(prompt=prompt, **kwargs)
        text_func = lambda response: response["choices"][0]["text"]
    else:
        create_func = lambda prompt: openai.ChatCompletion.create(messages=[{"role": "user", "content": prompt}], **kwargs)
        text_func = lambda response: response["choices"][0]["delta"].get("content", "") # ["message"]["content"]
    
    st.session_state["history"].append("")
    for response in create_func(prompt):
        st.session_state["history"][-1] += text_func(response)
        with container.container():
            st.markdown(st.session_state["history"][-1])