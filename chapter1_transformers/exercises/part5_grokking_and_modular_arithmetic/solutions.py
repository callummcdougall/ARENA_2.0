# %%

import torch as t
import torch.nn.functional as F
import numpy as np

from pathlib import Path
import os

import plotly.express as px
import plotly.graph_objects as go

from typing import List, Tuple, Union, Optional
from fancy_einsum import einsum
import einops
from tqdm import tqdm

from transformer_lens import HookedTransformer, HookedTransformerConfig, utils

from part5_grokking_and_modular_arithmetic.my_utils import *
import part5_grokking_and_modular_arithmetic.tests as tests

device = t.device("cuda" if t.cuda.is_available() else "cpu")

import sys, os
# Make sure exercises are in the path
chapter = r"chapter1_transformers"
exercises_dir = Path(f"{os.getcwd().split(chapter)[0]}/{chapter}/exercises").resolve()
section_dir = exercises_dir / "part5_grokking_and_modular_arithmetic"
if str(exercises_dir) not in sys.path: sys.path.append(str(exercises_dir))

# print(os.getcwd())

root = Path(section_dir / 'Grokking/saved_runs')
large_root = Path(section_dir / 'Grokking/large_files')

MAIN = __name__ == "__main__"

if MAIN:
    from IPython import get_ipython
    ipython = get_ipython()
    ipython.run_line_magic("load_ext", "autoreload")
    ipython.run_line_magic("autoreload", "2")


# !git clone https://github.com/neelnanda-io/Grokking.git
# os.mkdir(large_root)
# !pip install gdown
# !gdown "1OtbM0OGQCtGHjvSz-7q-7FkL2ReXIpmH&confirm=t" -O Grokking/large_files/full_run_data.pth

# %%

p = 113

cfg = HookedTransformerConfig(
    n_layers = 1,
    d_vocab = p+1,
    d_model = 128,
    d_mlp = 4 * 128,
    n_heads = 4,
    d_head = 128 // 4,
    n_ctx = 3,
    act_fn = "relu",
    normalization_type = None,
    device = device
)

model = HookedTransformer(cfg)

# %%

if MAIN:
    full_run_data = t.load(large_root / 'full_run_data.pth')
    state_dict = full_run_data["state_dicts"][400]

    model = load_in_state_dict(model, state_dict)
    # model = fix_order_of_attn_calc(model)

# %%

if MAIN:
    # Helper variables
    W_O = model.W_O[0]
    W_K = model.W_K[0]
    W_Q = model.W_Q[0]
    W_V = model.W_V[0]
    W_in = model.W_in[0]
    W_out = model.W_out[0]
    W_pos = model.W_pos
    W_E = model.W_E[:-1]
    final_pos_resid_initial = model.W_E[-1] + W_pos[2]
    W_U = model.W_U[:, :-1]

    print('W_O  ', tuple(W_O.shape))
    print('W_K  ', tuple(W_K.shape))
    print('W_Q  ', tuple(W_Q.shape))
    print('W_V  ', tuple(W_V.shape))
    print('W_in ', tuple(W_in.shape))
    print('W_out', tuple(W_out.shape))
    print('W_pos', tuple(W_pos.shape))
    print('W_E  ', tuple(W_E.shape))
    print('W_U  ', tuple(W_U.shape))

# %%

if MAIN:
    all_data = t.tensor([(i, j, p) for i in range(p) for j in range(p)]).to(device)
    labels = t.tensor([fn(i, j) for i, j, _ in all_data]).to(device)
    original_logits, cache = model.run_with_cache(all_data)
    # Final position only, also remove the logits for `=`
    original_logits = original_logits[:, -1, :-1]
    original_loss = cross_entropy_high_precision(original_logits, labels)
    print(f"Original loss: {original_loss.item()}")

# %%

if MAIN:
    attn_mat = cache['pattern', 0][:, :, 2]
    neuron_acts_post = cache['post', 0][:, -1]
    neuron_acts_pre = cache['pre', 0][:, -1]

    assert attn_mat.shape == (p*p, cfg.n_heads, 3)
    assert neuron_acts_post.shape == (p*p, cfg.d_mlp)
    assert neuron_acts_pre.shape == (p*p, cfg.d_mlp)

# %%

if MAIN:
    # Get the first three positional embedding vectors
    W_pos_x, W_pos_y, W_pos_equals = model.W_pos

    # Look at the difference between positional embeddings; show they are symmetric
    def compare_tensors(v, w):
        return ((v-w).pow(2).sum()/v.pow(2).sum().sqrt()/w.pow(2).sum().sqrt()).item()
    print('Difference in position embeddings', compare_tensors(W_pos_x, W_pos_y))
    print('Cosine similarity of position embeddings', t.cosine_similarity(W_pos_x, W_pos_y, dim=0).item())

    # Compare N(x, y) and N(y, x)
    neuron_acts_square = neuron_acts_post.reshape(p, p, d_mlp)
    print('Difference in neuron activations for (x,y) and (y,x): {:.3e}'.format(
        compare_tensors(
            neuron_acts_square, 
            einops.rearrange(neuron_acts_square, "x y d_mlp -> y x d_mlp")
        )
    ))

# %%

if MAIN:
    imshow(attn_mat.mean(0), xaxis='Position', yaxis='Head', title='Average Attention by source position and head', text_auto=".3f")

# %%

if MAIN:
    W_logit = W_out @ W_U

    # Note - W_OV is 3D becuase first dim is head_idx, but the `@` operator detects this 
    # and treats W_OV as a batch of matrices, which is exactly # what we want.
    W_OV = W_V @ W_O
    W_neur = W_E @ W_OV @ W_in

    W_QK = W_Q @ W_K.transpose(-1, -2)
    W_attn = final_pos_resid_initial @ W_QK @ W_E.T / (cfg.d_head ** 0.5)

    assert W_logit.shape == (cfg.d_mlp, cfg.d_vocab - 1)
    assert W_neur.shape == (cfg.n_heads, cfg.d_vocab - 1, cfg.d_mlp)
    assert W_attn.shape == (cfg.n_heads, cfg.d_vocab - 1)

# %%

if MAIN:
    # Ignore attn from 2 -> 2
    attn_mat = attn_mat[:, :, :2]

    # Rearrange attn_mat, so the first two dims represent (x, y) in modular arithmetic equation
    attn_mat_sq = einops.rearrange(attn_mat, "(x y) head seq -> x y head seq", x=p)

    inputs_heatmap(
        attn_mat_sq[..., 0],
        title=f'Attention score for heads at position 0',
        animation_frame=2,
        animation_name='head'
    )

# %%

if MAIN:
    # Rearrange activations, so the first two dims represent (x, y) in modular arithmetic equation
    neuron_acts_post_sq = einops.rearrange(neuron_acts_post, "(x y) d_mlp -> x y d_mlp", x=p)
    neuron_acts_pre_sq = einops.rearrange(neuron_acts_pre, "(x y) d_mlp -> x y d_mlp", x=p)

    top_k = 3
    inputs_heatmap(
        neuron_acts_post_sq[..., :top_k], 
        title=f'Activations for first {top_k} neurons',
        animation_frame=2,
        animation_name='Neuron'
    )

# %%

if MAIN:
    top_k = 5
    animate_multi_lines(
        W_neur[..., :top_k], 
        y_index = [f'head {hi}' for hi in range(4)],
        labels = {'x':'Input token', 'value':'Contribution to neuron'},
        snapshot='Neuron',
        title=f'Contribution to first {top_k} neurons via OV-circuit of heads (not weighted by attn)'
    )

# %%

if MAIN:
    lines(
        W_attn,
        labels = [f'head {hi}' for hi in range(4)],
        xaxis='Input token',
        yaxis='Contribution to attn score',
        title=f'Contribution to attention score (pre-softmax) for each head'
    )

# %%

def make_fourier_basis(p: int) -> Tuple[t.Tensor, List[str]]:
    '''
    Returns a pair `fourier_basis, fourier_basis_names`, where `fourier_basis` is
    a `(p, p)` tensor whose rows are Fourier components and `fourier_basis_names`
    is a list of length `p` containing the names of the Fourier components (e.g. 
    `["const", "cos 1", "sin 1", ...]`). You may assume that `p` is odd.
    '''
    # Define a grid for the Fourier basis vecs (we'll normalize them all at the end)
    # Note, the first vector is just the constant wave
    fourier_basis = t.ones(p, p)
    fourier_basis_names = ['Const']
    for i in range(1, p // 2 + 1):
        # Define each of the cos and sin terms
        fourier_basis[2*i-1] = t.cos(2*t.pi*t.arange(p)*i/p)
        fourier_basis[2*i] = t.sin(2*t.pi*t.arange(p)*i/p)
        fourier_basis_names.extend([f'cos {i}', f'sin {i}'])
    # Normalize vectors, and return them
    fourier_basis /= fourier_basis.norm(dim=1, keepdim=True)
    print(fourier_basis[13][5].item())
    return fourier_basis.to(device), fourier_basis_names

fourier_basis, fourier_basis_names = make_fourier_basis(p)

if MAIN:
    tests.test_make_fourier_basis(make_fourier_basis)

    fig = animate_lines(
        fourier_basis, 
        snapshot_index=fourier_basis_names, 
        snapshot='Fourier Component', 
        title='Graphs of Fourier Components (Use Slider)'
    )

# %%

if MAIN:
    imshow(fourier_basis @ fourier_basis.T)

# %%

def fft1d(x: t.Tensor) -> t.Tensor:
    '''
    Returns the 1D Fourier transform of `x`,
    which can be a vector or a batch of vectors.

    x.shape = (..., p)
    '''
    return x @ fourier_basis.T

if MAIN:
    tests.test_fft1d(fft1d)

    v = sum([
        fourier_basis[4],
        fourier_basis[15]/5,
        fourier_basis[67]/10
    ])

    line(v, xaxis='Vocab basis', title='Example periodic function')
    line(fft1d(v), xaxis='Fourier Basis', title='Fourier Transform of example function', hover=fourier_basis_names)

# %%

def fourier_2d_basis_term(i: int, j: int) -> t.Tensor:
    '''
    Returns the 2D Fourier basis term corresponding to the outer product of the
    `i`-th component of the 1D Fourier basis in the `x` direction and the `j`-th
    component of the 1D Fourier basis in the `y` direction.

    Returns a 2D tensor of length `(p, p)`.
    '''
    return (fourier_basis[i][:, None] * fourier_basis[j][None, :])

if MAIN:
    tests.test_fourier_2d_basis_term(fourier_2d_basis_term)

    x_term = 4
    y_term = 6

    inputs_heatmap(
        fourier_2d_basis_term(x_term, y_term).T,
        title=f"2D Fourier Basis term {fourier_basis_names[x_term]}x {fourier_basis_names[y_term]}y"
    )

# %%

def fft2d(tensor: t.Tensor) -> t.Tensor:
    '''
    Retuns the components of `tensor` in the 2D Fourier basis.

    Asumes that the input has shape `(p, p, ...)`, where the
    last dimensions (if present) are the batch dims.
    Output has the same shape as the input.
    '''
    # fourier_basis[i] is the i-th basis vector, which we want to multiply along
    return einops.einsum(
        tensor, fourier_basis, fourier_basis,
        "px py ..., i px, j py -> i j ..."
    )

if MAIN:
    tests.test_fft2d(fft2d)

    example_fn = sum([
        fourier_2d_basis_term(4, 6), 
        fourier_2d_basis_term(14, 46)/3,
        fourier_2d_basis_term(97, 100)/6
    ])

    inputs_heatmap(example_fn.T, title=f"Example periodic function")

    imshow_fourier(
        fft2d(example_fn),
        title='Example periodic function in 2D Fourier basis'
    )

# %%


if MAIN:
    # Apply Fourier transformation
    attn_mat_fourier_basis = fft2d(attn_mat_sq)

    # Plot results
    imshow_fourier(
        attn_mat_fourier_basis[..., 0], 
        title=f'Attention score for heads at position 0, in Fourier basis',
        animation_frame=2,
        animation_name='head'
    )

# %%

if MAIN:
    neuron_acts_post_fourier_basis = fft2d(neuron_acts_post_sq)

    top_k = 3
    imshow_fourier(
        neuron_acts_post_fourier_basis[..., :top_k], 
        title=f'Activations for first {top_k} neurons',
        animation_frame=2,
        animation_name='Neuron'
    )

# %%

def fft1d_given_dim(tensor: t.Tensor, dim: int) -> t.Tensor:
    '''
    Performs 1D FFT along the given dimension (not necessarily the last one).
    '''
    return fft1d(tensor.transpose(dim, -1)).transpose(dim, -1)

if MAIN:
    W_neur_fourier = fft1d_given_dim(W_neur, dim=1)

    top_k = 5
    animate_multi_lines(
        W_neur_fourier[..., :top_k], 
        y_index = [f'head {hi}' for hi in range(4)],
        labels = {'x':'Fourier component', 'value':'Contribution to neuron'},
        snapshot='Neuron',
        hover=fourier_basis_names,
        title=f'Contribution to first {top_k} neurons via OV-circuit of heads (not weighted by attn), in Fourier basis'
    )

# %%

if MAIN:
    lines(
        fft1d(W_attn), 
        labels = [f'head {hi}' for hi in range(4)],
        xaxis='Input token', 
        yaxis = 'Contribution to attn score',
        title=f'Contribution to attn score (pre-softmax) for each head, in Fourier Basis', 
        hover=fourier_basis_names
    )







# %% Section 2: Circuit and Feature Analysis


if MAIN:
    line(
        (fourier_basis @ model.W_E[:-1]).pow(2).sum(1), 
        hover=fourier_basis_names,
        title='Norm of embedding of each Fourier Component',
        xaxis='Fourier Component',
        yaxis='Norm'
    )

# %%

if MAIN:
    imshow_div(fourier_basis @ W_E)

# %%

if MAIN:
    top_k = 5
    inputs_heatmap(
        neuron_acts_post_sq[..., :top_k], 
        title=f'Activations for first {top_k} neurons',
        animation_frame=2,
        animation_name='Neuron'
    )
    imshow_fourier(
        neuron_acts_post_fourier_basis[..., :top_k], 
        title=f'Activations for first {top_k} neurons',
        animation_frame=2,
        animation_name='Neuron'
    )

# %%

if MAIN:
    # Center activations
    neuron_acts_centered = neuron_acts_post_sq - neuron_acts_post_sq.mean((0, 1), keepdim=True)

    # Take 2D Fourier transform
    neuron_acts_centered_fourier = fft2d(neuron_acts_centered)

    # Plot
    imshow_fourier(
        neuron_acts_centered_fourier.pow(2).mean(-1),
        title=f"Norms of 2D Fourier components of centered neuron activations",
    )

# %%

if MAIN:

    from sklearn.linear_model import LinearRegression

    # Choose a particular frequency, and get the corresponding cosine basis vector
    k = 42
    idx = 2 * k - 1
    vec = fourier_basis[idx]

    # Get ReLU function values
    relu_func_values = F.relu(0.5 * (p ** -0.5) + vec[None, :] + vec[:, None])

    # Get terms we'll be using to approximate it
    # Note we're including the constant term here
    data = t.stack([
        fourier_2d_basis_term(i, j)
        for (i, j) in [(0, 0), (idx, 0), (0, idx), (idx, idx)]
    ], dim=-1)

    # Reshape, and convert to numpy
    data = utils.to_numpy(data.reshape(p*p, 4))
    relu_func_values = utils.to_numpy(relu_func_values.flatten())

    # Fit a linear model (we don't need intercept because we have const Fourier basis term)
    reg = LinearRegression(fit_intercept=False).fit(data, relu_func_values)
    coefs = reg.coef_
    eqn = "ReLU(0.5 + cos(wx) + cos(wy) ≈ {:.3f}*const + {:.3f}*cos(wx) + {:.3f}*cos(wy) + {:.3f}*cos(wx)cos(wy)".format(*coefs)
    r2 = reg.score(data, relu_func_values)
    print(eqn)
    print("")
    print(f"r2: {r2:.3f}")

    # Run the regression again, but without the quadratic term
    data = data[:, :3]
    reg = LinearRegression().fit(data, relu_func_values)
    coefs = reg.coef_
    bias = reg.intercept_
    r2 = reg.score(data, relu_func_values)
    print(f"r2 (no quadratic term): {r2:.3f}")

# %%



def arrange_by_2d_freqs(tensor):
    '''
    Takes a tensor of shape (p, p, ...) and returns a tensor of shape
    (p//2, 3, 3, ...) representing the Fourier coefficients sorted by
    frequency (each slice contains const, linear and quadratic terms).
    '''
    idx_2d_y = []
    idx_2d_x = []
    for freq in range(1, p//2 + 1):
        idx_1d = [0, 2*freq-1, 2*freq]
        idx_2d_x.append([idx_1d for _ in range(3)])
        idx_2d_y.append([[i]*3 for i in idx_1d])
    return tensor[idx_2d_y, idx_2d_x]


def find_neuron_freqs(
    fourier_neuron_acts: t.Tensor
) -> Tuple[t.Tensor, t.Tensor]:
    '''
    Returns the tensors `neuron_freqs` and `neuron_frac_explained`, 
    containing the frequencies that explain the most variance of each 
    neuron and the fraction of variance explained, respectively.
    '''
    fourier_neuron_acts_by_freq = arrange_by_2d_freqs(fourier_neuron_acts)
    assert fourier_neuron_acts_by_freq.shape == (p//2, 3, 3, d_mlp)

    # Sum squares of all frequency coeffs, for each neuron
    square_of_all_terms = einops.reduce(
        fourier_neuron_acts.pow(2),
        "x_coeff y_coeff neuron -> neuron",
        "sum"
    )

    # Sum squares just corresponding to const+linear+quadratic terms,
    # for each frequency, for each neuron
    square_of_each_freq = einops.reduce(
        fourier_neuron_acts_by_freq.pow(2),
        "freq x_coeff y_coeff neuron -> freq neuron",
        "sum"
    )

    # Find the freq explaining most variance for each neuron
    # (and the fraction of variance explained)
    neuron_variance_explained, neuron_freqs = square_of_each_freq.max(0)
    neuron_frac_explained = neuron_variance_explained / square_of_all_terms

    # The actual frequencies count up from k=1, not 0!
    neuron_freqs += 1

    return neuron_freqs, neuron_frac_explained


if MAIN:
    neuron_freqs, neuron_frac_explained = find_neuron_freqs(neuron_acts_centered_fourier)
    key_freqs, neuron_freq_counts = t.unique(neuron_freqs, return_counts=True)

# %%

if MAIN:
    fraction_of_activations_positive_at_posn2 = (cache['pre', 0][:, -1] > 0).float().mean(0)

    scatter(
        x=neuron_freqs, 
        y=neuron_frac_explained,
        xaxis="Neuron frequency", 
        yaxis="Frac explained", 
        colorbar_title="Frac positive",
        title="Fraction of neuron activations explained by key freq",
        color=utils.to_numpy(fraction_of_activations_positive_at_posn2)
    )

# %%

if MAIN:
    # To represent that they are in a special sixth cluster, we set the 
    # frequency of these neurons to -1
    neuron_freqs[neuron_frac_explained < 0.85] = -1.
    key_freqs_plus = t.concatenate([key_freqs, -key_freqs.new_ones((1,))])

    for i, k in enumerate(key_freqs_plus):
        print(f'Cluster {i}: freq k={k}, {(neuron_freqs==k).sum()} neurons')

# %%

if MAIN:
    fourier_norms_in_each_cluster = []
    for freq in key_freqs:
        fourier_norms_in_each_cluster.append(
            einops.reduce(
                neuron_acts_centered_fourier.pow(2)[..., neuron_freqs==freq], 
                'batch_y batch_x neuron -> batch_y batch_x', 
                'mean'
            )
        )

    imshow_fourier(
        t.stack(fourier_norms_in_each_cluster), 
        title=f'Norm of 2D Fourier components of neuron activations in each cluster',
        facet_col=0,
        facet_labels=[f"Freq={freq}" for freq in key_freqs]
    )

# %%

if MAIN:
    imshow_fourier(
        einops.reduce(
            neuron_acts_centered_fourier[..., neuron_freqs==-1].pow(2), 
            'x y neuron -> x y', 
            'sum'
        ))

# %%


def project_onto_direction(batch_vecs: t.Tensor, v: t.Tensor) -> t.Tensor:
    '''
    Returns the component of each vector in `batch_vecs` in the
    direction of `v`.

    batch_vecs.shape = (n, ...)
    v.shape = (n,)
    '''
    return v[:, None] @ (v @ batch_vecs)[None, ...]

if MAIN:
    tests.test_project_onto_direction(project_onto_direction)


def project_onto_direction(batch_vecs: t.Tensor, v: t.Tensor) -> t.Tensor:
    '''
    Returns the component of each vector in `batch_vecs` in the
    direction of `v`.

    batch_vecs.shape = (n, ...)
    v.shape = (n,)
    '''

    # Get tensor of components of each vector in v-direction
    components_in_v_dir = einops.einsum(
        batch_vecs, v,
        "n ..., n -> ..."
    )

    # Use these components as coefficients of v in our projections
    return einops.einsum(
        components_in_v_dir, v,
        "..., n -> n ..."
    )

if MAIN:
    tests.test_project_onto_direction(project_onto_direction)


def project_onto_frequency(batch_vecs: t.Tensor, freq: int) -> t.Tensor:
    '''
    Returns the projection of each vector in `batch_vecs` onto the
    2D Fourier basis directions corresponding to frequency `freq`.
    
    batch_vecs.shape = (p**2, ...)
    '''
    assert batch_vecs.shape[0] == p**2

    return sum([
        project_onto_direction(
            batch_vecs,
            fourier_2d_basis_term(i, j).flatten(),
        )
        for i in [0, 2*freq-1, 2*freq] for j in [0, 2*freq-1, 2*freq]
    ])

if MAIN:
    tests.test_project_onto_frequency(project_onto_frequency)

# %%

if MAIN:
    logits_in_freqs = []

    for freq in key_freqs:

        # Get all neuron activations corresponding to this frequency
        filtered_neuron_acts = neuron_acts_post[:, neuron_freqs==freq]

        # Project onto const/linear/quadratic terms in 2D Fourier basis
        filtered_neuron_acts_in_freq = project_onto_frequency(filtered_neuron_acts, freq)

        # Calcluate new logits, from these filtered neuron activations
        logits_in_freq = filtered_neuron_acts_in_freq @ W_logit[neuron_freqs==freq]

        logits_in_freqs.append(logits_in_freq)
    
    # We add on neurons in the always firing cluster, unfiltered
    logits_always_firing = neuron_acts_post[:, neuron_freqs==-1] @ W_logit[neuron_freqs==-1]
    logits_in_freqs.append(logits_always_firing)

    # Print new losses
    print('Loss with neuron activations ONLY in key freq (incl always firing cluster)\n{:.6e}\n'.format( 
        test_logits(
            sum(logits_in_freqs), 
            bias_correction=True, 
            original_logits=original_logits
        )
    ))
    print('Loss with neuron activations ONLY in key freq (excl always firing cluster)\n{:.6e}\n'.format( 
        test_logits(
            sum(logits_in_freqs[:-1]), 
            bias_correction=True, 
            original_logits=original_logits
        )
    ))
    print('Original loss\n{:.6e}'.format(original_loss))

# %%

if MAIN:
    print('Loss with neuron activations excluding none:     {:.9f}'.format(original_loss.item()))
    for c, freq in enumerate(key_freqs_plus):
        print('Loss with neuron activations excluding freq={}:  {:.9f}'.format(
            freq, 
            test_logits(
                sum(logits_in_freqs) - logits_in_freqs[c], 
                bias_correction=True, 
                original_logits=original_logits
            )
        ))


# %%

if MAIN:
    imshow_fourier(
        einops.reduce(neuron_acts_centered_fourier.pow(2), 'y x neuron -> y x', 'mean'), 
        title='Norm of Fourier Components of Neuron Acts'
    )

    # Rearrange logits, so the first two dims represent (x, y) in modular arithmetic equation
    original_logits_sq = einops.rearrange(original_logits, "(x y) z -> x y z", x=p)
    original_logits_fourier = fft2d(original_logits_sq)

    imshow_fourier(
        einops.reduce(original_logits_fourier.pow(2), 'y x z -> y x', 'mean'), 
        title='Norm of Fourier Components of Logits'
    )

# %%

def get_trig_sum_directions(k: int) -> Tuple[t.Tensor, t.Tensor]:
    '''
    Given frequency k, returns the normalized vectors in the 2D Fourier basis 
    representing the directions:

        cos(ω_k * (x + y))
        sin(ω_k * (x + y))

    respectively.
    '''
    cosx_cosy_direction = fourier_2d_basis_term(2*k-1, 2*k-1)
    sinx_siny_direction = fourier_2d_basis_term(2*k, 2*k)
    sinx_cosy_direction = fourier_2d_basis_term(2*k, 2*k-1)
    cosx_siny_direction = fourier_2d_basis_term(2*k-1, 2*k)

    cos_xplusy_direction = (cosx_cosy_direction - sinx_siny_direction) / np.sqrt(2)
    sin_xplusy_direction = (sinx_cosy_direction + cosx_siny_direction) / np.sqrt(2)

    return cos_xplusy_direction, sin_xplusy_direction

if MAIN:
    tests.test_get_trig_sum_directions(get_trig_sum_directions)

# %%

if MAIN:
    trig_logits = []

    for k in key_freqs:

        cos_xplusy_direction, sin_xplusy_direction = get_trig_sum_directions(k)

        cos_xplusy_projection = project_onto_direction(
            original_logits,
            cos_xplusy_direction.flatten()
        )

        sin_xplusy_projection = project_onto_direction(
            original_logits,
            sin_xplusy_direction.flatten()
        )

        trig_logits.extend([cos_xplusy_projection, sin_xplusy_projection])

    trig_logits = sum(trig_logits)

    print(f'Loss with just x+y components: {test_logits(trig_logits, True, original_logits):.4e}')
    print(f"Original Loss: {original_loss:.4e}")

# %%

if MAIN:
    US = W_logit @ fourier_basis.T

    imshow_div(
        US,
        x=fourier_basis_names, 
        yaxis='Neuron',
        title='W_logit in the Fourier Basis'
    )

# %%

if MAIN:
    US_sorted = t.concatenate([
        US[neuron_freqs==freq] for freq in key_freqs_plus
    ])
    hline_positions = np.cumsum([(neuron_freqs == freq).sum().item() for freq in key_freqs]).tolist() + [cfg.d_mlp]
    
    imshow_div(
        US_sorted,
        x=fourier_basis_names, 
        yaxis='Neuron',
        title='W_logit in the Fourier Basis (rearranged by neuron cluster)',
        hline_positions = hline_positions,
        hline_labels = [f"Cluster: {freq=}" for freq in key_freqs] + ["No freq"],
    )


# %%

if MAIN:
    cos_components = []
    sin_components = []

    for k in key_freqs:
        logits_projected_onto_cos_dir = neuron_acts_post_sq @ US[:, 2*k-1]
        logits_projected_onto_sin_dir = neuron_acts_post_sq @ US[:, 2*k]

        cos_components.append(fft2d(logits_projected_onto_cos_dir))
        sin_components.append(fft2d(logits_projected_onto_sin_dir))
        
    for title, components in zip(['Cosine', 'Sine'], [cos_components, sin_components]):
        imshow_fourier(
            t.stack(components),
            title=f'{title} components of neuron activations in Fourier basis',
            animation_frame=0,
            animation_name="Frequency",
            animation_labels=key_freqs.tolist()
        )

# %%

if MAIN:
    cos_components = []
    sin_components = []

    for k in key_freqs:
        σu_sin = US[:, 2*k]
        σu_cos = US[:, 2*k-1]

        logits_in_cos_dir = neuron_acts_post_sq @ σu_cos
        logits_in_sin_dir = neuron_acts_post_sq @ σu_sin

        cos_components.append(fft2d(logits_in_cos_dir))
        sin_components.append(fft2d(logits_in_sin_dir))
        
    for title, components in zip(['Cosine', 'Sine'], [cos_components, sin_components]):
        imshow_fourier(
            t.stack(components),
            title=f'{title} components of neuron activations in Fourier basis',
            animation_frame=0,
            animation_name="Frequency",
            animation_labels=key_freqs.tolist()
        )


# %%


# %% TRAINING MODEL (BONUS)

# from torch.utils.data import TensorDataset, DataLoader, random_split

# def train(frac_train: float):

#     dataset = TensorDataset(all_data, labels)

#     len_trainset = int(frac_train * len(dataset))
#     len_testset = len(dataset) - len_trainset

#     trainset, testset = random_split(dataset, [len_trainset, len_testset])

#     trainloader = DataLoader(trainset, batch_size=2048, shuffle=True)
#     testloader = DataLoader(testset, batch_size=2048, shuffle=False)

#     def cross_entropy_high_precision(logits, labels):
#         logprobs = F.log_softmax(logits.to(t.float64), dim=-1)
#         prediction_logprobs = t.gather(logprobs, index=labels[:, None], dim=-1)
#         loss = -t.mean(prediction_logprobs)
#         return loss

#     model = HookedTransformer(cfg) # .train().to(device)
#     optimizer = t.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1.0) # 1e-4

#     train_loss_list = []
#     test_loss_list = []

#     epochs = 3000

#     for epoch in range(epochs):

#         model.train()
#         train_loss = 0
#         for i, (tokens, xplusy) in enumerate(trainloader):
#             logits = model(tokens)
#             logits = logits[:, -1, :-1]
#             loss = cross_entropy_high_precision(logits, xplusy)
#             loss.backward()
#             optimizer.step()
#             optimizer.zero_grad()
#             train_loss += loss.item() * tokens.size(0)

#         train_loss_list.append(train_loss / len(trainset))

#         model.eval()
#         test_loss = 0
#         with t.inference_mode():
#             for i, (tokens, xplusy) in enumerate(testloader):
#                 logits = model(tokens)
#                 logits = logits[:, -1, :-1]
#                 loss = cross_entropy_high_precision(logits, xplusy)
#                 test_loss += loss.item() * tokens.size(0)
            
#             test_loss_list.append(test_loss / len(testset))

#             if loss.item() < 3e-6:
#                 break
        
#         if epoch % 50 == 0:
#             print(f"Epoch {epoch:02}, train loss: {train_loss_list[-1]:.6f}, test loss: {test_loss_list[-1]:.6f}")

#         if test_loss < 3e-6: # 1e-2
#             break

#     px.line(y=[train_loss_list, test_loss_list], labels={"x": "Epochs", "y": "Loss"}).show()

#     return train_loss_list, test_loss_list

# %%

from dataclasses import dataclass

class MetricResults:
    '''
    Class to hold the results of a particular metric, calculated
    over the training period of a model.
    '''
    def __init__(self, fn: Callable, name: str, desc: str, results: list[t.Tensor]):
        self.fn = fn
        self.name = name if name else fn.__name__
        self.desc = desc if desc else fn.__doc__
        self.results: t.Tensor = t.stack(results).to(device)
    
    def __repr__(self):
        return f"{self.name}: {self.desc}"


class MetricCache:
    def __init__(self, epochs, state_dicts, model) -> None:
        '''
        Define objects which are required to calculate metrics for a model over time.
        '''
        self.epochs = epochs
        self.state_dicts = state_dicts
        self.model = model
        self.cache = {}

    def get_metrics(self, metric_fn, metric_name="", metric_desc="", metric_kwargs={}, reset=False):
        '''
        Calculate a metric for a model over time, and cache the results.
        If reset=True, then the metric will be recalculated.        
        '''
        metric_name = metric_name if metric_name else metric_fn.__name__
        metric_desc = metric_desc if metric_desc else metric_fn.__doc__
        if reset or (metric_name not in self.cache):
            results = []
            for sd in tqdm(self.state_dicts):
                model = load_in_state_dict(self.model, sd)
                out = metric_fn(model, **metric_kwargs).detach()
                results.append(out)
            self.cache[metric_name] = MetricResults(metric_fn, metric_name, metric_desc, results)
        else:
            print(f"Already cached metric {metric_name!r}. Use `reset=True` to recalculate.")

    def __repr__(self):
        return "\n".join([str(r) for r in metric_cache.cache.values()])
    
    def __getitem__(self, key: str) -> MetricResults:
        return self.cache[key]


if MAIN:
    
    epochs = full_run_data['epochs']
    state_dicts = full_run_data['state_dicts']

    plot_metric = partial(lines, x=epochs, xaxis='Epoch', log_y=True)

    metric_cache = MetricCache(
        epochs=epochs,
        state_dicts=state_dicts,
        model=model
    )

    def test_loss(model: HookedTransformer):
        '''Cross entropy loss on test set (without bias correction).
        '''
        logits = model(all_data)[:, -1, :-1]
        return test_logits(logits, False, mode='test')
    
    def train_loss(model: HookedTransformer):
        '''Cross entropy loss on training set (without bias correction).
        '''
        logits = model(all_data)[:, -1, :-1]
        return test_logits(logits, False, mode='train')

    metric_cache.get_metrics(test_loss)
    metric_cache.get_metrics(train_loss)
    
# %%

def excl_loss(model: HookedTransformer, key_freqs: list) -> float:
    '''
    Returns the excluded loss (i.e. subtracting the components of logits
    corresponding to cos(w_k(x+y)) and sin(w_k(x+y)), for each frequency
    k in key_freqs.
    '''
    excl_loss_list = []
    logits = model(all_data)[:, -1, :-1]

    for freq in key_freqs:
        cos_xplusy_direction, sin_xplusy_direction = get_trig_sum_directions(freq)

        logits_cos_xplusy = project_onto_direction(
            logits,
            cos_xplusy_direction.flatten()
        )

        logits_sin_xplusy = project_onto_direction(
            logits,
            sin_xplusy_direction.flatten()
        )

        logits_excl = logits - logits_cos_xplusy - logits_sin_xplusy

        loss = test_logits(logits_excl, bias_correction=False, mode='train').item()

        excl_loss_list.append(loss)

    return t.tensor(excl_loss_list)


if MAIN:
    metric_cache.get_metrics(excl_loss, metric_kwargs=dict(key_freqs=key_freqs))

    lines(
        t.concat([
            metric_cache['excl_loss'].results.T, 
            metric_cache['train_loss'].results[None, :],  
            metric_cache['test_loss'].results[None, :]
        ], axis=0), 
        labels=[f'excl {freq}' for freq in key_freqs]+['train', 'test'], 
        title='Excluded Loss for each trig component',
        log_y=True,
        x=full_run_data['epochs'],
        xaxis='Epoch',
        yaxis='Loss'
    )

# %%

def fourier_embed(model: HookedTransformer):
    '''
    Returns the norm of the Fourier components of the embedding.
    '''
    W_E_fourier = fourier_basis @ model.W_E[:-1]
    return einops.reduce(W_E_fourier.pow(2), 'vocab d_model -> vocab', 'sum')

if MAIN:
    # Plot every 200 epochs so it doesn't take too long
    metric_cache.get_metrics(fourier_embed)

    animate_lines(
        metric_cache['fourier_embed'].results[::2],
        snapshot_index = epochs[::2],
        snapshot='Epoch',
        hover=fourier_basis_names,
        animation_group='x',
        title='Norm of Fourier Components in the Embedding Over Training'
    )

# %%

def embed_SVD(model: HookedTransformer):
    '''
    Returns vector S, where W_E = U @ diag(S) @ V.T in singular value decomp.
    '''
    U, S, V = t.svd(model.W_E[:, :-1])
    return S


if MAIN:
    metric_cache.get_metrics(embed_SVD)
    
    animate_lines(
        metric_cache['embed_SVD'].results,
        snapshot_index = epochs,
        snapshot='Epoch',
        title='Singular Values of the Embedding During Training',
        xaxis='Singular Number',
        yaxis='Singular Value',
    )

# %%

def tensor_trig_ratio(model: HookedTransformer, mode: str):
    '''
    Returns the fraction of variance of the (centered) activations which
    is explained by the Fourier directions corresponding to cos(ω(x+y))
    and sin(ω(x+y)) for all the key frequencies.
    '''
    logits, cache = model.run_with_cache(all_data)
    logits = logits[:, -1, :-1]
    # match mode:
    #     case 'neuron_pre': tensor = cache['pre', 0][:, -1]
    #     case 'neuron_post': tensor = cache['post', 0][:, -1]
    #     case 'logit': tensor = logits
    #     case _: raise ValueError(f"{mode} is not a valid mode")
    if mode == "neuron_pre": 
        tensor = cache['pre', 0][:, -1]
    elif mode == "neuron_post":
        tensor = cache['post', 0][:, -1]
    elif mode == "logit":
        tensor = logits
    else:
        raise ValueError(f"{mode} is not a valid mode")
    
    tensor_centered = tensor - einops.reduce(tensor, 'xy index -> 1 index', 'mean')
    tensor_var = einops.reduce(tensor_centered.pow(2), 'xy index -> index', 'sum')
    tensor_trig_vars = []
    
    for freq in key_freqs:
        cos_xplusy_direction, sin_xplusy_direction = get_trig_sum_directions(freq)
        cos_xplusy_projection_var = project_onto_direction(
            tensor_centered, cos_xplusy_direction.flatten()
        ).pow(2).sum(0)
        sin_xplusy_projection_var = project_onto_direction(
            tensor_centered, sin_xplusy_direction.flatten()
        ).pow(2).sum(0)
    
        tensor_trig_vars.extend([cos_xplusy_projection_var, sin_xplusy_projection_var])

    return sum(tensor_trig_vars) / tensor_var


if MAIN:
    for mode in ['neuron_pre', 'neuron_post', 'logit']:
        metric_cache.get_metrics(
            tensor_trig_ratio, 
            metric_name=f'{mode}_trig_ratio', 
            metric_kwargs=dict(mode=mode)
        )

# %%

if MAIN:
    lines_list = []
    line_labels = []
    for mode in ['neuron_pre', 'neuron_post', 'logit']:
        tensor = metric_cache[f"{mode}_trig_ratio"].results
        lines_list.append(einops.reduce(tensor, 'epoch index -> epoch', 'mean'))
        line_labels.append(f"{mode}_trig_frac")

    plot_metric(
        lines_list, 
        labels=line_labels, 
        log_y=False,
        yaxis='Ratio',
        title='Fraction of logits and neurons explained by trig terms',
    )
    
# %%

if MAIN:
    plot_metric(
        lines_list,
        labels=line_labels, 
        log_y=True,
        yaxis='Ratio',
        title='Fraction of logits and neurons explained by trig terms',
    )

# %%

def get_frac_explained(model: HookedTransformer):
    _, cache = model.run_with_cache(all_data, return_type=None)

    returns = []

    for neuron_type in ['pre', 'post']:
        neuron_acts = cache[neuron_type, 0][:, -1].clone().detach()
        neuron_acts_centered = neuron_acts - neuron_acts.mean(0)
        neuron_acts_fourier = fft2d(
            einops.rearrange(neuron_acts_centered, "(x y) neuron -> x y neuron", x=p)
        )

        # Calculate the sum of squares over all inputs, for each neuron
        square_of_all_terms = einops.reduce(
            neuron_acts_fourier.pow(2), "x y neuron -> neuron", "sum"
        )

        frac_explained = t.zeros(d_mlp).to(device)
        frac_explained_quadratic_terms = t.zeros(d_mlp).to(device)

        for freq in key_freqs_plus:
            # Get Fourier activations for neurons in this frequency cluster
            # We arrange by frequency (i.e. each freq has a 3x3 grid with const, linear & quadratic terms)
            acts_fourier = arrange_by_2d_freqs(neuron_acts_fourier[..., neuron_freqs==freq])

            # Calculate the sum of squares over all inputs, after filtering for just this frequency
            # Also calculate the sum of squares for just the quadratic terms in this frequency
            if freq==-1:
                squares_for_this_freq = squares_for_this_freq_quadratic_terms = einops.reduce(
                    acts_fourier[:, 1:, 1:].pow(2), "freq x y neuron -> neuron", "sum"
                )
            else:
                squares_for_this_freq = einops.reduce(
                    acts_fourier[freq-1].pow(2), "x y neuron -> neuron", "sum"
                )
                squares_for_this_freq_quadratic_terms = einops.reduce(
                    acts_fourier[freq-1, 1:, 1:].pow(2), "x y neuron -> neuron", "sum"
                )

            frac_explained[neuron_freqs==freq] = squares_for_this_freq / square_of_all_terms[neuron_freqs==freq]
            frac_explained_quadratic_terms[neuron_freqs==freq] = squares_for_this_freq_quadratic_terms / square_of_all_terms[neuron_freqs==freq]

        returns.extend([frac_explained, frac_explained_quadratic_terms])

    frac_active = (neuron_acts > 0).float().mean(0)

    return t.nan_to_num(t.stack(returns + [neuron_freqs, frac_active], axis=0))


if MAIN:
    metric_cache.get_metrics(get_frac_explained)





# %%

if MAIN:
    frac_explained_pre = metric_cache['get_frac_explained'].results[:, 0]
    frac_explained_quadratic_pre = metric_cache['get_frac_explained'].results[:, 1]
    frac_explained_post = metric_cache['get_frac_explained'].results[:, 2]
    frac_explained_quadratic_post = metric_cache['get_frac_explained'].results[:, 3]
    neuron_freqs_ = metric_cache['get_frac_explained'].results[:, 4]
    frac_active = metric_cache['get_frac_explained'].results[:, 5]

    animate_scatter(
        t.stack([frac_explained_quadratic_pre, frac_explained_quadratic_post], dim=1)[:200:5],
        color=neuron_freqs_[:200:5], 
        color_name='freq',
        snapshot='epoch',
        snapshot_index=epochs[:200:5],
        xaxis='Quad ratio pre',
        yaxis='Quad ratio post',
        color_continuous_scale='viridis',
        title='Fraction of variance explained by quadratic terms (up to epoch 20K)'
    )

    animate_scatter(
        t.stack([neuron_freqs_, frac_explained_pre, frac_explained_post], dim=1)[:200:5],
        color=frac_active[:200:5],
        color_name='frac_active',
        snapshot='epoch',
        snapshot_index=epochs[:200:5],
        xaxis='Freq',
        yaxis='Frac explained',
        hover=list(range(d_mlp)),
        color_continuous_scale='viridis',
        title='Fraction of variance explained by this frequency (up to epoch 20K)'
    )

# %%

def avg_attn_pattern(model: HookedTransformer):
    _, cache = model.run_with_cache(all_data, return_type=None)
    return einops.reduce(
        cache['pattern', 0][:, :, 2], 'batch head pos -> head pos', 'mean'
    )

if MAIN:
    metric_cache.get_metrics(avg_attn_pattern)

# %%

if MAIN:
    avg_attn_pattern_results = metric_cache['avg_attn_pattern'].results

    imshow_div(
        avg_attn_pattern_results[::5],
        animation_frame=0,
        animation_name='epoch',
        animation_labels=epochs[::5],
        title='Avg attn by position and head, snapped every 500 epochs', 
        xaxis='Pos', 
        yaxis='Head',
        zmax=0.5,
        zmin=0.0,
        color_continuous_scale='Blues',
        text_auto='.3f',
    )

# %%

if MAIN:
    lines(
        (avg_attn_pattern_results[:, :, 0] - avg_attn_pattern_results[:, :, 1]).T,
        labels=[f"head {i}" for i in range(4)],
        x=epochs,
        xaxis='Epoch',
        yaxis='Average difference',
        title='Attention to pos 0 - pos 1 by head over training'
    )

# %%

def trig_loss(model: HookedTransformer, mode: str = 'all'):
    '''
    Returns the cross entropy loss from the model, after projecting its logit output
    onto the directions cos(ω(x+y)) and sin(ω(x+y)) for all the key frequencies.
    '''
    logits = model(all_data)[:, -1, :-1]
    
    trig_logits = []
    for freq in key_freqs:
        cos_xplusy_dir, sin_xplusy_dir = get_trig_sum_directions(freq)
        cos_xplusy_proj = project_onto_direction(logits, cos_xplusy_dir.flatten())
        sin_xplusy_proj = project_onto_direction(logits, sin_xplusy_dir.flatten())
        trig_logits.extend([cos_xplusy_proj, sin_xplusy_proj])
    trig_logits = sum(trig_logits)

    return test_logits(
        trig_logits, bias_correction=True, original_logits=logits, mode=mode
    )


if MAIN:
    metric_cache.get_metrics(trig_loss)
    metric_cache.get_metrics(trig_loss, metric_name='trig_loss_train', metric_kwargs=dict(mode='train'))

    line_labels = ['test_loss', 'train_loss', 'trig_loss', 'trig_loss_train']
    plot_metric(
        [metric_cache[lab].results for lab in line_labels], 
        labels=line_labels, title='Different losses over training'
    )
    plot_metric(
        [metric_cache['test_loss'].results / metric_cache['trig_loss'].results], 
        title='Ratio of trig and test loss'
    )

# %%

if MAIN:
    def sum_sq_weights(model):
        '''Returns the sum of squared weights for each of the model's params.'''
        return t.Tensor([param.pow(2).sum().item() for name, param in model.named_parameters()])
    
    metric_cache.get_metrics(sum_sq_weights)

    plot_metric(
        metric_cache['sum_sq_weights'].results.T, 
        title='Sum of squared weights for each parameter',
        # Take only the end of each parameter name for brevity
        labels=[name.split('.')[-1] for name, param in model.named_parameters()],
        log_y=False
    )
    plot_metric(
        [einops.reduce(metric_cache['sum_sq_weights'].results, 'epoch param -> epoch', 'sum')], 
        title='Total sum of squared weights',
        log_y=False
    )

# %%









# Note - I originally thought this was pointless to observe, because surely summing over 
# neuron activations for pre vs post is dumb if all post are positive anyway (obviously they'll
# destructively interfere for pre but not post). But this is forgetting the fact that we're
# working in the Fourier basis, and that we're expecting positive coefficients (see section gamma > 0).

if MAIN:
    # Get neuron activations, centered
    neuron_acts_pre_centered = neuron_acts_pre_sq - neuron_acts_pre_sq.mean(0, keepdim=True)
    neuron_acts_post_centered = neuron_acts_post_sq - neuron_acts_post_sq.mean(0, keepdim=True)

    fourier_neuron_acts_pre = fft2d(neuron_acts_pre_centered)
    fourier_neuron_acts_post = fft2d(neuron_acts_post_centered)

    # Zero the linear terms, to make it easier to see the quadratic terms forming
    quadratic_mask = t.outer(t.arange(p) > 0, t.arange(p) > 0).float().unsqueeze(-1).to(device)
    # Sum the pre and post activations
    fourier_norms_neuron_pre = einops.reduce(
        fourier_neuron_acts_pre * quadratic_mask, 
        'x y neuron -> x y', 'sum', x=p
    )
    fourier_norms_neuron_post = einops.reduce(
        fourier_neuron_acts_post * quadratic_mask, 
        'x y neuron -> x y', 'sum', x=p
    )
    imshow_fourier(
        t.stack([fourier_norms_neuron_pre, fourier_norms_neuron_post], axis=2),
        facet_col=2,
        facet_labels=['Pre', 'Post'], 
        title='Norm of 2D Fourier components of neuron pre and post activation (excl const)'
    )

# %%

# TODO - there's still a small bug here, because the results I'm getting are slightly different
# to Neel's Colab page. It looks like I just have a smaller scale factor, oddly.

# I tried changing fourier_neuron_acts_pre = fft2d(neuron_acts_pre_sq), and oddly that made it
# much too small (i.e. incorrect in the opposite way). I guess because the denominator also included
# the constant term, making it too big.

if MAIN:
    neuron_norms_pre = t.zeros(d_mlp).to(device)
    neuron_norms_post = t.zeros(d_mlp).to(device)
    fourier_norms_neuron_pre_rearranged = arrange_by_2d_freqs(fourier_neuron_acts_pre)
    fourier_norms_neuron_post_rearranged = arrange_by_2d_freqs(fourier_neuron_acts_post)
    for freq in key_freqs:
        neuron_norms_pre[neuron_freqs==freq] = fourier_norms_neuron_pre_rearranged[freq-1, 1:, 1:, neuron_freqs==freq].pow(2).sum((0, 1))
        neuron_norms_post[neuron_freqs==freq] = fourier_norms_neuron_post_rearranged[freq-1, 1:, 1:, neuron_freqs==freq].pow(2).sum((0, 1))
    neuron_ratio_pre = neuron_norms_pre / fourier_neuron_acts_pre.pow(2).sum((0, 1))
    neuron_ratio_post = neuron_norms_post / fourier_neuron_acts_post.pow(2).sum((0, 1))

    scatter(
        utils.to_numpy(neuron_ratio_pre[neuron_freqs!=-1]), 
        utils.to_numpy(neuron_ratio_post[neuron_freqs!=-1]), 
        color=utils.to_numpy(neuron_freqs[neuron_freqs!=-1]), 
        color_continuous_scale='viridis', 
        hover_name=np.arange(d_mlp)[utils.to_numpy(neuron_freqs)!=-1],
        # xaxis_range=[0, 1],
        # yaxis_range=[0, 1],
    )

# %%

def scatter(x, y, **kwargs):
    fig = px.scatter(x=utils.to_numpy(x.flatten()), y=utils.to_numpy(y.flatten()), **{k: v for k, v in kwargs.items() if k not in ['xaxis_range', 'yaxis_range']})
    if "xaxis_range" in kwargs:
        fig.update_xaxes(range=kwargs["xaxis_range"])
    if "yaxis_range" in kwargs:
        fig.update_yaxes(range=kwargs["yaxis_range"])
    fig.show()

if MAIN:
    neuron_norms_pre = t.zeros(d_mlp, device='cuda')
    neuron_norms = t.zeros(d_mlp, device='cuda')
    for freq in key_freqs:
        neuron_norms_pre[neuron_freqs==freq] = fourier_neuron_acts_pre[2*freq-1:2*freq+1, 2*freq-1:2*freq+1, neuron_freqs==freq].pow(2).sum((0, 1))
        neuron_norms[neuron_freqs==freq] = fourier_neuron_acts_post[2*freq-1:2*freq+1, 2*freq-1:2*freq+1, neuron_freqs==freq].pow(2).sum((0, 1))
    neuron_ratio_pre = neuron_norms_pre/einops.reduce(fourier_neuron_acts_pre.pow(2), 'x y neuron -> neuron', 'sum')
    neuron_ratio = neuron_norms/einops.reduce(fourier_neuron_acts_post.pow(2), 'x y neuron -> neuron', 'sum')
    scatter(
        utils.to_numpy(neuron_ratio_pre[neuron_freqs!=-1]), 
        utils.to_numpy(neuron_ratio_post[neuron_freqs!=-1]), 
        color=utils.to_numpy(neuron_freqs[neuron_freqs!=-1]), 
        labels={"x": "Pre", "y": "Post"},
        xaxis_range=[0, 1],
        yaxis_range=[0, 1],
        color_continuous_scale='viridis', 
        hover_name=np.arange(d_mlp)[utils.to_numpy(neuron_freqs)!=-1]
    )
# %%
